# 一：基础知识

<https://blog.csdn.net/m0_37809890/article/details/92441427>

1：机器学习的应用主要集中于：分类和问题求解:

划线：画最好的线

没有免费午餐定理：如果

2:分类：

监督学习：输入数据被称为“训练数据”，每组训练数据，有明确的标识或结果。常用于分类问题或回归问题，常用算法如逻辑回归和反向传递神经网络。（决策树，朴素贝叶斯，逻辑回归，KNN，随机森林，神经网络，SVN）

无监督学习：数据并不被特别标识，学习模型是为了推出数据的一些内在结构。应用场景：关联规则的学习，聚类等。常见算法：Apriori,
k-Means

半监督学习：分类包括回归、聚类、降维

强化学习（RL）

3：机器学习流程

数据预处理：清理，集成，转换，规约

模型学习：保留交叉验证，k重交叉验证

模型评估：过拟合，欠拟合

预测

视频学习笔记

西瓜书

![](media/b92073c2d577a3f163a9e1fe6c8b3056.png)

![](media/ae7cbf347f3e2fe2e3bb7d34b94e9c83.png)

Numpy

Pandas

Anaconda

Python

## 第一章：绪论

数据集：100个习惯

样本：1个西瓜

特征向量：颜色，大小，响度

属性：颜色

设计算法得到模型

>   有监督学习

>   分类：二分类为题（瓜栽还是不摘）多分类（市场上有哪些瓜）

>   回归：预测下年西瓜啥时间是最便宜的

>   无监督学习

>   聚类：大小

>   区别：有监督学习有老师教，无监督学习没有老师教，有监督学习通过已有的训练样本得到模型，在利用模型将所有输入映射为相应输出。无监督学习没有任何训练样本，而是直接对数据进行建模。聚类。

预测

假设空间

归纳偏好：

奥卡姆剃刀：选择简单的那个

没有免费的午餐定理（NFL定理）：无论学习算法a多聪明、学习算法b多笨拙，它们的期望性能都相同。

## 第二章：模型评估

随着训练样本的增加，平均训练误差会增大，平均测试误差会减小

1：评估方法

留出法：

交叉验证

自助法

2：评估指标

准确率

错误率

查准率（P）

查全率（R）

调和均值F1

ROC曲线

AUC

CLL

3：比较检验

![](media/cae1da0c8624ec58c1ba3f2648a837b8.png)

1：一种训练集一种算法

测试集的保留方法：留出法（部分数据用来训练，部分数据用来预测，三七分）

交叉验证法：K折交叉验证

自助法：

验证集：调参

性能度量：

均方误差

![](media/ec80c48f81174e8aa82eb11039185bb3.png)

错误路与精度

![](media/0c82d519e03cd1899572341c58f5ceb4.png)

查准率和查全率：（样本分布不均衡，使用错误率不准确了）

![](media/377b381c095de5e5228de51d2df37721.png)

PR曲线：

比较集中曲线的好坏：

方法一：查全率相同，查准率高的好

方法二：比较面积

ROC曲线：

ROC曲线判断好坏：

越凸越好

![](media/ae0a3560b5b1702bc387ef6edb0a1dc3.png)

一种训练集多种算法

多种训练集一种算法

一个测试集集中算法

多个测试集一种算法

离散随机变量的函数分布

卡方分布

T分布

正态分布处理为t分布

多种测试集两种算法：交叉验证t检验

一个测试集两种算法：McNemer检验

多个测试集多种算法：Friedman检验与Nemeny后续检验

## 第三章：线性模型 

### 1：线性回归

![](media/1eff9f0ed6279b43ed6c331de0a9707e.png)

使用最小二乘法对w和b进行评估

![](media/5c93ad737e267bd5e14192637b4b5dd6.png)

![](media/e2ec9b646e11641b0b607c9aeac8a397.png)

![](media/7ad3fdb41a1e4c325a6d07f35d1bd1c4.png)

### 2：广义线性回归

现实中很多问题是非线性的，将线性回归的预测值做一个非线性的函数变化去逼近真实值

![](media/acfde3ac63450590164758b8ff1d4971.png)

联系函数为指数函数式，成为对数线性回归

![](media/eb2a8633dac108362058e242584936dc.png)

### 3：逻辑斯蒂回归—二分类问题

![](media/eeb65290dc79d9d12ae8d5eb14b3be51.png)

![](media/fec815ca914dfbd124844e9b2747407d.png)

### 4：多分类学习

![](media/804cd5d99db41dd1dee9050d2e2c112c.png)

## 第四章：支持向量机

### 1：概念

确定一个分类超平面，从而将不同的数据分割开

![](media/5c927f644961412ab67a7ee8892afb00.png)

![](media/df4235c87502fba87b15746f63e280a4.png)

![](media/cf8ea10cc80266f783ccb0faaf6e7f33.png)

![](media/bc7a8bb469528dcc8ea408dd7f2becbc.png)

![](media/6856457029a89eb62f6ed666beaa3912.png)

使用现成的或则拉格朗日乘子法

### 2：分类

线性可分支持向量机

线性支持向量机

非线性支持向量机

### 3：核函数

将非线性转化为线性问题

一般由经验给出

正定核——正定矩阵

多项式核函数

高斯核函数

## 第五章：神经网络

MP神经元模型

![](media/2a75ac08f389bd2f7d552b9aaf60e4b7.png)

### 单层感知机

只拥有一层MP神经元

![](media/bc97d8f33f2d162c2342ee41b709df00.png)

### 多层前馈神经网络

![](media/b80446f9bd67744a38d181a78d3204c3.png)

误差逆传播算法（BP）

![](media/153b55e238e388170ec2e9d220c06c24.png)

BP面临的问题

1.  结构学习问题

2.  初始化问题

3.  步长设置问题

4.  权值与阈值的更新问题

5.  过拟合问题

### 深层神经网络

![](media/40e9f8293356a404404c8199e2957377.png)

## 第六章：决策树学习

根据某些特征的判别对数据进行分类

### 最佳划分的度量问题

不纯度量

![](media/6c8dba52b7811d441ade603b25d68aa1.png)

![](media/a0ccaa9088ebbb38b1dd36ac49e78d4c.png)

增益率

C4.5的启发式方法

![](media/d6c5ba9041a36cdcdc2f44bdfbb14228.png)

![](media/8656f587b7347ce8d7e689fedf45ab28.png)

例：根据天气，温度等划分决策树

计算各信息增益，最大的是OutLook，根据OutLook划分

![](media/5e192059f11421b7a245cc159cad3109.png)

判断Sunny有2+和3-不是叶子节点，再划分，计算其他的信息增益，发现Humidity最大，

![](media/082b43fd40b16dc28a95e54808f194e3.png)

### 处理缺失属性问题

![](media/6842dfff53ae8af22318041b60a520cb.png)

过拟合

>   预剪枝

>   后剪枝（实践中更直接）

## 第七章：贝叶斯

贝叶斯定理：

![](media/ba0fa8075353637f542d1ed1fc602c4b.png)

朴素贝叶斯定理：假设输入的不同特征之间是独立的。

应用：文本分类，垃圾邮件过滤，病人分类，拼音检查

极大似然估计MLE：模型已定，参数未知，

最大后验概率MAP：获得对实验数据中无法直接观察到的量的点估计。MAP就是多个作为因子的先验概率P(θ)。或者，也可以反过来，认为MLE是把先验概率P(θ)认为等于1，即认为θ是均匀分布。

因为MLE
只考虑训练数据拟合程度没有考虑先验知识，把错误点也加入模型中，导致过拟合。

### 基础知识

![](media/505aa41bf03a69213dfa98dc263825ba.png)

![](media/30529a98cb559bd309a9fcf368013dc5.png)

### 朴素贝叶斯分类器

条件独立

![](media/70dc5f211b31571fd1415ecca39eaa3d.png)

例：天气等

![](media/736a5d20c3865295a79aec97bcf69abf.png)

改进

1.  处理算法：结构扩展

2.  处理数据：

    面向特征（特征选择，特征加权）

    面向实例（实例选择，实例加权）

## 第八章：最近邻学习kNN

积极学习：有显式的训练过程，都是在训练阶段对样本进行学习处理，构建分类模型

消极学习(lazy
learning)：没有显式训练过程，训练阶段只是把训练样本保存起来，建模工作

>   延迟到工作阶段才进行处理，如最近邻学习

![](media/818450233c992ee25681951a2754017d.png)

### 近邻索引问题

几乎所有计算花费都在索引近邻上，使用最多的是通过计算待测样本与每一个训练样本之间的距离，然后基于距离排序，选择距离最短的k个训练样本作为待测赝本。

![](media/ec9ff3f7471ec45a03f1e10d0d9259a6.png)

### 维度灾害问题

如果目标函数仅依赖于很多属性中的几个时，样本间的距离会被大量不相关的属性所支配，从而导致相关属性的值很接近的样本相距很远。

解决方案：属性加权，属性选择（加权为0和1）

### 领域大小问题

基于经验直接给出，基于数据自动学习

后验概率问题

计算效率问题

归纳偏置问题’

## 第八章：集成学习

构件并结合多个学习期来完成学习任务，有时又称多分类器系统。先产生一组个体学习器，再用某种策略将他们结合起来。

个体强依赖必须串行生成序列——Boosting

个体不强依赖可并行化——bagging和随机森林

### Boosting

Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习**误差率表现来更新训练样本的权重**，使得之前弱学习器1**学习误差率高的训练样本点的权重变高**，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

AdaBoost

AdaBoosting方式每次使用的是全部的样本，每轮训练改变样本的权重。下一轮训练的目标是找到一个函数f
来拟合上一轮的残差。当残差足够小或者达到设置的最大迭代次数则停止。Boosting会减小在上一轮训练正确的样本的权重，增大错误样本的权重。（对的残差小，错的残差大）分类器更加关注分错的样本点

GBDT

回归树，梯度上升决策树，核心在于累加所有树的结果，一个人是30岁，先用20岁去拟合，然后发现损失有10岁，再用6岁去拟合剩下的损失，发现差距还有4岁，继续迭代，拟合的岁数的误差就会越来越小，不断拟合残差

### Bagging与随机森林（RF）

Bgging：在原始数据集中有放回的选取，均匀取样，

## 第九章：聚类

距离计算

明科夫斯基距离（曼哈顿距离和欧式距离）

K均值算法（kmeans）

学习向量量化

高斯混合聚类：采用概率模型来表达

基于密度的聚类

层次聚类

k-means：K是指所要聚的cluster的数量，means是指每一个cluster都有一个中心点（质心），这个质心是cluster中所有点的平均值，分别计算样本中每个点与K个质心的欧式距离，离哪个质心最近，这个点就被划到哪一类中。继续选出新的质心，如果新的质心与旧的质心的差距小于一定得阈值，则不再更新。

GMM（高斯混合模型）：所有的分布可以看做是多个高斯（正态）分布综合起来的结果。这样一来，任何分布都可以分成多个高斯分布来表示。通过样本找到K个高斯分布的期望和方差，那么K个高斯模型就确定了。在聚类的过程中，不会明确的指定一个样本属于哪一类，而是计算这个样本在某个分布中的可能性。

第十章：强化学习

机器学习三大分支：无监督学习，监督学习，强化学习

![](media/25c697c1e2b273ec2e3128554cdb57f0.png)

矩阵形式：

>   X11，x12，。。。。。X1d

>   X21，X22，。。。。。X2d

>   。。。

>   Xm1，。。。。。。Xmd

![](media/fd085df5b3ea5d0ed219c33034b7d003.png)

![](media/4218aa3e3b6781f9799e8c0223bad6fd.png)

令导数等于0求出最优解，就像求极大似然估计

多元线性回归

![](media/28c9856e4a99e15a4e3354368ef39a28.png)

对数线性回归

![](media/2db0c5d7234d870243bec81ef88e86fb.png)

3:对数几率回归（逻辑回归）

分类问题：将线性模型转换为分类问题

需要将在线性方程上添加一个方程，转换为分类问题

二分类问题：Y={0,1}

对数几率函数（logistic function Sigmoid函数）

![](media/c2b1677abc49d99b5a46833dd9c30760.png)

![](media/a5b8a477cc1383ec8098e84093ac3108.png)

## 机器学习期末考试

<https://wenku.baidu.com/view/05c8a1caed630b1c58eeb50e.html>

样本（样例）：数据的特定实例，为xn，分为有标签样本和无标签样本，

有标签样本包含特征和标签，无标签包含特征，不包含标签

标签：要预测的事务，为y

特征：输入变量，为x

激活函数：将前一层所有神经元激活值的加权和输入到一个非线性函数中，然后向下一层传递该函数的输出值（典型的非线性）。

机器学习：机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。旨在准确的预测

统计学习：基于数据构建概率统计模型并运用模型对数据进行预测与分析

推测变量之间的关系。

机器学习的一般步骤：搜集数据，数据预处理，选择模型，训练模型，评估模型，参数微调，预测

样本属性的主要类型：连续性，二值离散，多值离散，混合类型

信息增益：信息不确定信减少的程度。信息增益越大，区分样本的能力越强，越具有代表性，说明该特征越重要。比如阴天对于下雨就是很重要的特征。

聚类分析有哪些重要的距离度量方法：欧式距离，曼哈顿距离，切比雪夫，余弦相似度，皮尔森相似度
